{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bf06d38d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /home/siya/.cache/kagglehub/datasets/japeralrashid/xr-bones-dataset-for-bone-fracture-detection/versions/5\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"japeralrashid/xr-bones-dataset-for-bone-fracture-detection\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a94a1f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset location: /home/siya/.cache/kagglehub/datasets/japeralrashid/xr-bones-dataset-for-bone-fracture-detection/versions/5\n",
      "\n",
      "Directory structure:\n",
      "5/\n",
      "    YOLODataSet/\n",
      "        xr_bones.yaml\n",
      "        xr.yaml\n",
      "        images/\n",
      "            train/\n",
      "                XR_SHOULDER_positive_3530.png\n",
      "                XR_ELBOW_positive_366.png\n",
      "                XR_ELBOW_positive_597.png\n",
      "                ... and 23848 more files\n",
      "            val/\n",
      "                XR_FINGER_negative_1710.png\n",
      "                XR_ELBOW_negative_761.png\n",
      "                XR_ELBOW_negative_927.png\n",
      "                ... and 997 more files\n",
      "        labels/\n",
      "            train/\n",
      "                XR_ELBOW_positive_523.txt\n",
      "                XR_SHOULDER_negative_1146.txt\n",
      "                XR_FINGER_negative_1022.txt\n",
      "                ... and 21498 more files\n",
      "            val/\n",
      "                XR_FINGER_negative_2228.txt\n",
      "                XR_HAND_positive_413.txt\n",
      "                XR_SHOULDER_negative_3653.txt\n",
      "                ... and 997 more files\n",
      "\n",
      "Found 0 label CSV files\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import glob\n",
    "import json\n",
    "\n",
    "# Path to your downloaded dataset\n",
    "dataset_path = \"/home/siya/.cache/kagglehub/datasets/japeralrashid/xr-bones-dataset-for-bone-fracture-detection/versions/5\"\n",
    "print(f\"Dataset location: {dataset_path}\")\n",
    "\n",
    "# List all directories and files\n",
    "print(\"\\nDirectory structure:\")\n",
    "for root, dirs, files in os.walk(dataset_path, topdown=True):\n",
    "    level = root.replace(dataset_path, '').count(os.sep)\n",
    "    indent = ' ' * 4 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 4 * (level + 1)\n",
    "    for f in files[:3]:  # Show only first 3 files to avoid clutter\n",
    "        print(f\"{subindent}{f}\")\n",
    "    if len(files) > 3:\n",
    "        print(f\"{subindent}... and {len(files)-3} more files\")\n",
    "\n",
    "# Check CSV files\n",
    "csv_files = glob.glob(os.path.join(dataset_path, \"*_labels.csv\"))\n",
    "print(f\"\\nFound {len(csv_files)} label CSV files\")\n",
    "\n",
    "# Examine one CSV file to understand its structure\n",
    "if csv_files:\n",
    "    sample_csv = csv_files[0]\n",
    "    df = pd.read_csv(sample_csv)\n",
    "    print(f\"\\nStructure of {os.path.basename(sample_csv)}:\")\n",
    "    print(df.head())\n",
    "    print(f\"\\nTotal entries: {len(df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "548fe3ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YOLO Configuration:\n",
      "names:\n",
      "- XR_ELBOW_positive\n",
      "- XR_FINGER_positive\n",
      "- XR_FOREARM_positive\n",
      "- XR_HAND_positive\n",
      "- XR_SHOULDER_positive\n",
      "- XR_ELBOW_negative\n",
      "- XR_FINGER_negative\n",
      "- XR_FOREARM_negative\n",
      "- XR_HAND_negative\n",
      "- XR_SHOULDER_negative\n",
      "nc: 10\n",
      "train: /YOLODataSet/images/train\n",
      "val: /YOLODataSet/images/val\n",
      "\n",
      "\n",
      "Dataset Statistics:\n",
      "Training images: 23851\n",
      "Validation images: 1000\n",
      "Training labels: 21501\n",
      "Validation labels: 1000\n",
      "\n",
      "Sample images:\n",
      "Image 1: XR_SHOULDER_positive_3530.png\n",
      "Image 2: XR_ELBOW_positive_366.png\n",
      "Image 3: XR_ELBOW_positive_597.png\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import yaml\n",
    "import glob\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Dataset paths\n",
    "dataset_path = \"/home/siya/.cache/kagglehub/datasets/japeralrashid/xr-bones-dataset-for-bone-fracture-detection/versions/5\"\n",
    "yolo_config_path = os.path.join(dataset_path, \"YOLODataSet\", \"xr_bones.yaml\")\n",
    "\n",
    "# Load YOLO configuration\n",
    "with open(yolo_config_path, 'r') as f:\n",
    "    yolo_config = yaml.safe_load(f)\n",
    "\n",
    "print(\"YOLO Configuration:\")\n",
    "print(yaml.dump(yolo_config, indent=2))\n",
    "\n",
    "# Check dataset statistics\n",
    "train_images = glob.glob(os.path.join(dataset_path, \"YOLODataSet\", \"images\", \"train\", \"*.png\"))\n",
    "val_images = glob.glob(os.path.join(dataset_path, \"YOLODataSet\", \"images\", \"val\", \"*.png\"))\n",
    "train_labels = glob.glob(os.path.join(dataset_path, \"YOLODataSet\", \"labels\", \"train\", \"*.txt\"))\n",
    "val_labels = glob.glob(os.path.join(dataset_path, \"YOLODataSet\", \"labels\", \"val\", \"*.txt\"))\n",
    "\n",
    "print(f\"\\nDataset Statistics:\")\n",
    "print(f\"Training images: {len(train_images)}\")\n",
    "print(f\"Validation images: {len(val_images)}\")\n",
    "print(f\"Training labels: {len(train_labels)}\")\n",
    "print(f\"Validation labels: {len(val_labels)}\")\n",
    "\n",
    "# Sample a few images to understand the data\n",
    "print(\"\\nSample images:\")\n",
    "for i, img_path in enumerate(train_images[:3]):\n",
    "    print(f\"Image {i+1}: {os.path.basename(img_path)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a07efcfc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting training data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing train images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23851/23851 [00:25<00:00, 942.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 23851 training samples\n",
      "\n",
      "Converting validation data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing val images: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:01<00:00, 930.83it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted 1000 validation samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "def yolo_to_qwen_format(dataset_path, output_dir, split='train', max_samples=None):\n",
    "    \"\"\"\n",
    "    Convert YOLO format dataset to Qwen3-VL format\n",
    "    \"\"\"\n",
    "    # Create output directories\n",
    "    os.makedirs(os.path.join(output_dir, 'images'), exist_ok=True)\n",
    "    \n",
    "    # Get image and label paths\n",
    "    image_dir = os.path.join(dataset_path, \"YOLODataSet\", \"images\", split)\n",
    "    label_dir = os.path.join(dataset_path, \"YOLODataSet\", \"labels\", split)\n",
    "    \n",
    "    image_paths = glob.glob(os.path.join(image_dir, \"*.png\"))\n",
    "    \n",
    "    if max_samples:\n",
    "        image_paths = image_paths[:max_samples]\n",
    "    \n",
    "    qwen_data = []\n",
    "    \n",
    "    for img_path in tqdm(image_paths, desc=f\"Processing {split} images\"):\n",
    "        # Get corresponding label file\n",
    "        img_name = os.path.basename(img_path)\n",
    "        label_name = os.path.splitext(img_name)[0] + \".txt\"\n",
    "        label_path = os.path.join(label_dir, label_name)\n",
    "        \n",
    "        # Read image to get dimensions\n",
    "        img = Image.open(img_path)\n",
    "        img_width, img_height = img.size\n",
    "        \n",
    "        # Copy image to output directory\n",
    "        output_img_path = os.path.join(output_dir, 'images', img_name)\n",
    "        shutil.copy2(img_path, output_img_path)\n",
    "        \n",
    "        # Process labels\n",
    "        bounding_boxes = []\n",
    "        has_fracture = False\n",
    "        \n",
    "        if os.path.exists(label_path):\n",
    "            with open(label_path, 'r') as f:\n",
    "                lines = f.readlines()\n",
    "            \n",
    "            for line in lines:\n",
    "                parts = line.strip().split()\n",
    "                if len(parts) >= 5:\n",
    "                    class_id = int(float(parts[0]))\n",
    "                    # YOLO format: center_x, center_y, width, height (normalized)\n",
    "                    center_x = float(parts[1]) * img_width\n",
    "                    center_y = float(parts[2]) * img_height\n",
    "                    bbox_width = float(parts[3]) * img_width\n",
    "                    bbox_height = float(parts[4]) * img_height\n",
    "                    \n",
    "                    # Convert to [x_min, y_min, x_max, y_max]\n",
    "                    x_min = center_x - (bbox_width / 2)\n",
    "                    y_min = center_y - (bbox_height / 2)\n",
    "                    x_max = center_x + (bbox_width / 2)\n",
    "                    y_max = center_y + (bbox_height / 2)\n",
    "                    \n",
    "                    # Clamp to image boundaries\n",
    "                    x_min = max(0, min(img_width, x_min))\n",
    "                    y_min = max(0, min(img_height, y_min))\n",
    "                    x_max = max(0, min(img_width, x_max))\n",
    "                    y_max = max(0, min(img_height, y_max))\n",
    "                    \n",
    "                    bounding_boxes.append([x_min, y_min, x_max, y_max])\n",
    "                    has_fracture = True\n",
    "        \n",
    "        # Determine anatomical region from filename\n",
    "        region = \"unknown\"\n",
    "        if \"ELBOW\" in img_name:\n",
    "            region = \"elbow\"\n",
    "        elif \"FINGER\" in img_name:\n",
    "            region = \"finger\"\n",
    "        elif \"FOREARM\" in img_name:\n",
    "            region = \"forearm\"\n",
    "        elif \"HAND\" in img_name:\n",
    "            region = \"hand\"\n",
    "        elif \"SHOULDER\" in img_name:\n",
    "            region = \"shoulder\"\n",
    "        \n",
    "        # Create Qwen3-VL formatted messages\n",
    "        if has_fracture:\n",
    "            prompt = f\"Identify and box all fracture regions in this {region} X-ray image. Return results in JSON format.\"\n",
    "            response = {\n",
    "                \"fracture_present\": True,\n",
    "                \"anatomical_region\": region,\n",
    "                \"bounding_boxes\": [\n",
    "                    {\"bbox_2d\": bbox, \"label\": \"fracture\", \"region\": region} \n",
    "                    for bbox in bounding_boxes\n",
    "                ],\n",
    "                \"diagnosis\": \"Fracture detected in the highlighted region.\"\n",
    "            }\n",
    "        else:\n",
    "            prompt = f\"Analyze this {region} X-ray image for any signs of fractures. Return results in JSON format.\"\n",
    "            response = {\n",
    "                \"fracture_present\": False,\n",
    "                \"anatomical_region\": region,\n",
    "                \"bounding_boxes\": [],\n",
    "                \"diagnosis\": \"No fractures detected in this X-ray image.\"\n",
    "            }\n",
    "        \n",
    "        # Create conversation format\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": prompt}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": json.dumps(response)\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        qwen_data.append({\n",
    "            \"messages\": messages,\n",
    "            \"image\": output_img_path,\n",
    "            \"metadata\": {\n",
    "                \"original_image\": img_path,\n",
    "                \"has_fracture\": has_fracture,\n",
    "                \"region\": region,\n",
    "                \"bounding_boxes\": bounding_boxes\n",
    "            }\n",
    "        })\n",
    "    \n",
    "    return qwen_data\n",
    "\n",
    "# Create output directory\n",
    "output_dir = \"./xr_bones_qwen_format\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Convert training and validation data\n",
    "print(\"Converting training data...\")\n",
    "train_data = yolo_to_qwen_format(dataset_path, output_dir, split='train')\n",
    "print(f\"Converted {len(train_data)} training samples\")\n",
    "\n",
    "print(\"\\nConverting validation data...\")\n",
    "val_data = yolo_to_qwen_format(dataset_path, output_dir, split='val')\n",
    "print(f\"Converted {len(val_data)} validation samples\")\n",
    "\n",
    "# Save dataset metadata\n",
    "dataset_info = {\n",
    "    \"total_train_samples\": len(train_data),\n",
    "    \"total_val_samples\": len(val_data),\n",
    "    \"anatomical_regions\": [\"elbow\", \"finger\", \"forearm\", \"hand\", \"shoulder\"],\n",
    "    \"created_at\": \"2025-11-13\"\n",
    "}\n",
    "\n",
    "with open(os.path.join(output_dir, \"dataset_info.json\"), 'w') as f:\n",
    "    json.dump(dataset_info, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ddefff87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating Hugging Face dataset with proper schema...\n",
      "Processing training data for dataset creation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 23851/23851 [00:00<00:00, 978580.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing validation data for dataset creation...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing data: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:00<00:00, 675628.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating validation dataset...\n",
      "Dataset created successfully:\n",
      "  Training samples: 23851\n",
      "  Validation samples: 1000\n",
      "\n",
      "Dataset structure verification:\n",
      "Train features: {'text_input': Value('string'), 'image_path': Value('string'), 'target_output': Value('string'), 'has_fracture': Value('bool'), 'region': Value('string'), 'original_image_path': Value('string')}\n",
      "Sample training example: {'text_input': 'Analyze this shoulder X-ray image for any signs of fractures. Return results in JSON format.', 'image_path': './xr_bones_qwen_format/images/XR_SHOULDER_positive_3530.png', 'target_output': '{\"fracture_present\": false, \"anatomical_region\": \"shoulder\", \"bounding_boxes\": [], \"diagnosis\": \"No fractures detected in this X-ray image.\"}', 'has_fracture': False, 'region': 'shoulder', 'original_image_path': '/home/siya/.cache/kagglehub/datasets/japeralrashid/xr-bones-dataset-for-bone-fracture-detection/versions/5/YOLODataSet/images/train/XR_SHOULDER_positive_3530.png'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be92744690c64730a2f007531e3b4529",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/23851 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0911ba9949a94071acb06c71f7f34dde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset saved to ./xr_bones_qwen_format/hf_dataset_fixed\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset, DatasetDict, Features, Value\n",
    "import json\n",
    "from PIL import Image\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "def create_hf_dataset_fixed(train_data, val_data):\n",
    "    \"\"\"\n",
    "    Create Hugging Face dataset with a simple, robust schema that avoids all nested structure issues\n",
    "    \"\"\"\n",
    "    # Define a super simple schema that will definitely work\n",
    "    features = Features({\n",
    "        \"text_input\": Value(\"string\"),           # User prompt text\n",
    "        \"image_path\": Value(\"string\"),           # Path to image file\n",
    "        \"target_output\": Value(\"string\"),        # Assistant response (JSON string)\n",
    "        \"has_fracture\": Value(\"bool\"),           # Whether fracture is present\n",
    "        \"region\": Value(\"string\"),               # Anatomical region\n",
    "        \"original_image_path\": Value(\"string\"), # Original image path\n",
    "    })\n",
    "    \n",
    "    # Process data to match the simple schema\n",
    "    def process_data(data_list):\n",
    "        processed = []\n",
    "        for item in tqdm(data_list, desc=\"Processing data\"):\n",
    "            try:\n",
    "                # Get user text from messages\n",
    "                user_text = \"\"\n",
    "                for content_item in item[\"messages\"][0][\"content\"]:\n",
    "                    if content_item[\"type\"] == \"text\":\n",
    "                        user_text = content_item[\"text\"]\n",
    "                \n",
    "                # Get assistant response\n",
    "                assistant_content = item[\"messages\"][1][\"content\"]\n",
    "                \n",
    "                # Extract metadata\n",
    "                has_fracture = item[\"metadata\"][\"has_fracture\"]\n",
    "                region = item[\"metadata\"][\"region\"]\n",
    "                original_image_path = item[\"metadata\"][\"original_image\"]\n",
    "                \n",
    "                processed.append({\n",
    "                    \"text_input\": user_text,\n",
    "                    \"image_path\": item[\"image\"],\n",
    "                    \"target_output\": assistant_content,\n",
    "                    \"has_fracture\": has_fracture,\n",
    "                    \"region\": region,\n",
    "                    \"original_image_path\": original_image_path,\n",
    "                })\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing item: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    # Process training and validation data\n",
    "    print(\"Processing training data for dataset creation...\")\n",
    "    train_processed = process_data(train_data)\n",
    "    \n",
    "    print(\"Processing validation data for dataset creation...\")\n",
    "    val_processed = process_data(val_data)\n",
    "    \n",
    "    # Create datasets with explicit features\n",
    "    print(\"Creating training dataset...\")\n",
    "    train_dataset = Dataset.from_list(train_processed, features=features)\n",
    "    \n",
    "    print(\"Creating validation dataset...\")\n",
    "    val_dataset = Dataset.from_list(val_processed, features=features)\n",
    "    \n",
    "    # Create DatasetDict\n",
    "    dataset = DatasetDict({\n",
    "        \"train\": train_dataset,\n",
    "        \"validation\": val_dataset\n",
    "    })\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "# Create Hugging Face dataset with fixed approach\n",
    "print(\"\\nCreating Hugging Face dataset with proper schema...\")\n",
    "dataset = create_hf_dataset_fixed(train_data, val_data)\n",
    "\n",
    "print(f\"Dataset created successfully:\")\n",
    "print(f\"  Training samples: {len(dataset['train'])}\")\n",
    "print(f\"  Validation samples: {len(dataset['validation'])}\")\n",
    "\n",
    "# Verify dataset structure\n",
    "print(\"\\nDataset structure verification:\")\n",
    "print(f\"Train features: {dataset['train'].features}\")\n",
    "print(f\"Sample training example: {dataset['train'][0]}\")\n",
    "\n",
    "# Save dataset for future use\n",
    "output_dir = \"./xr_bones_qwen_format\"\n",
    "os.makedirs(os.path.join(output_dir, \"hf_dataset_fixed\"), exist_ok=True)\n",
    "dataset.save_to_disk(os.path.join(output_dir, \"hf_dataset_fixed\"))\n",
    "print(f\"Dataset saved to {os.path.join(output_dir, 'hf_dataset_fixed')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bb0da5b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "HARDWARE SETUP\n",
      "============================================================\n",
      "CUDA available: True\n",
      "GPU: NVIDIA GeForce RTX 4080 Laptop GPU\n",
      "Total VRAM: 12.0 GB\n",
      "Current memory usage: 0.00 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory optimized. Current usage: 0.00 GB\n",
      "\n",
      "============================================================\n",
      "MODEL LOADING\n",
      "============================================================\n",
      "Loading Qwen3-VL-2B-Instruct with 4-bit quantization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model loaded successfully!\n",
      "Current memory usage: 1.47 GB\n",
      "\n",
      "Enabling gradient checkpointing...\n",
      "Preparing model for k-bit training...\n",
      "\n",
      "Configuring LoRA for medical imaging fine-tuning...\n",
      "Applying LoRA configuration...\n",
      "\n",
      "‚úÖ Model configuration complete!\n",
      "Trainable parameters: 34,865,152\n",
      "Total memory usage: 2.19 GB\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import Qwen3VLForConditionalGeneration, AutoProcessor\n",
    "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training\n",
    "import bitsandbytes as bnb\n",
    "import gc\n",
    "import os\n",
    "\n",
    "# Hardware verification\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"HARDWARE SETUP\")\n",
    "print(\"=\"*60)\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    total_vram = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
    "    print(f\"Total VRAM: {total_vram:.1f} GB\")\n",
    "    print(f\"Current memory usage: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "# Memory optimization\n",
    "def optimize_memory():\n",
    "    \"\"\"Optimize GPU memory usage\"\"\"\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    torch.cuda.set_per_process_memory_fraction(0.85)\n",
    "    print(f\"Memory optimized. Current usage: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "optimize_memory()\n",
    "\n",
    "# Model loading\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MODEL LOADING\")\n",
    "print(\"=\"*60)\n",
    "print(\"Loading Qwen3-VL-2B-Instruct with 4-bit quantization...\")\n",
    "\n",
    "model_name = \"Qwen/Qwen3-VL-2B-Instruct\"\n",
    "\n",
    "# Load model with 4-bit quantization\n",
    "model = Qwen3VLForConditionalGeneration.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=torch.float16,\n",
    "    device_map=\"auto\",\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "\n",
    "processor = AutoProcessor.from_pretrained(model_name)\n",
    "\n",
    "print(\"‚úÖ Model loaded successfully!\")\n",
    "print(f\"Current memory usage: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")\n",
    "\n",
    "# Enable gradient checkpointing and prepare for training\n",
    "print(\"\\nEnabling gradient checkpointing...\")\n",
    "model.gradient_checkpointing_enable()\n",
    "model.enable_input_require_grads()\n",
    "\n",
    "print(\"Preparing model for k-bit training...\")\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# Configure LoRA\n",
    "print(\"\\nConfiguring LoRA for medical imaging fine-tuning...\")\n",
    "lora_config = LoraConfig(\n",
    "    r=32,\n",
    "    lora_alpha=64,\n",
    "    target_modules=[\n",
    "        \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "        \"gate_proj\", \"up_proj\", \"down_proj\",\n",
    "    ],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "print(\"Applying LoRA configuration...\")\n",
    "model = get_peft_model(model, lora_config)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"\\n‚úÖ Model configuration complete!\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Total memory usage: {torch.cuda.memory_allocated()/1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aaa0f5e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data collator created successfully!\n",
      "‚úÖ Training configuration created successfully!\n",
      "Training parameters:\n",
      "  Batch size per device: 1\n",
      "  Gradient accumulation steps: 8\n",
      "  Effective batch size: 8\n",
      "  Learning rate: 2e-05\n",
      "  Epochs: 2\n"
     ]
    }
   ],
   "source": [
    "from transformers import DataCollatorWithPadding, TrainingArguments\n",
    "import torch\n",
    "from typing import Dict, List, Any\n",
    "from PIL import Image\n",
    "import json\n",
    "import numpy as np\n",
    "\n",
    "class MedicalImagingDataCollator:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "    \n",
    "    def __call__(self, features: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Fixed collator with proper image_grid_thw parameter for Qwen3-VL\n",
    "        \"\"\"\n",
    "        # Extract images and text inputs\n",
    "        images = []\n",
    "        texts = []\n",
    "        \n",
    "        for feature in features:\n",
    "            # Load image from path\n",
    "            image = Image.open(feature[\"image_path\"]).convert(\"RGB\")\n",
    "            images.append(image)\n",
    "            \n",
    "            # Create the conversation format\n",
    "            messages = [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": feature[\"text_input\"]}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": feature[\"target_output\"]\n",
    "                }\n",
    "            ]\n",
    "            \n",
    "            # Apply chat template\n",
    "            text = self.processor.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=True\n",
    "            )\n",
    "            texts.append(text)\n",
    "        \n",
    "        # Process all inputs together\n",
    "        inputs = self.processor(\n",
    "            images=images,\n",
    "            text=texts,\n",
    "            return_tensors=\"pt\",\n",
    "            padding=True,\n",
    "            truncation=True,\n",
    "            max_length=2048\n",
    "        )\n",
    "        \n",
    "        # Create labels for training (mask input tokens)\n",
    "        labels = inputs[\"input_ids\"].clone()\n",
    "        \n",
    "        # FIX: Add image_grid_thw parameter - CRITICAL FOR QWEN3-VL\n",
    "        batch_size = len(images)\n",
    "        # For standard Qwen3-VL images, use this grid size\n",
    "        image_grid_thw = torch.tensor([[1, 24, 24]] * batch_size, dtype=torch.long)\n",
    "        \n",
    "        # Return batch with ALL required parameters\n",
    "        return {\n",
    "            \"input_ids\": inputs[\"input_ids\"],\n",
    "            \"attention_mask\": inputs[\"attention_mask\"],\n",
    "            \"pixel_values\": inputs[\"pixel_values\"],\n",
    "            \"image_grid_thw\": image_grid_thw,  # This is the crucial fix\n",
    "            \"labels\": labels\n",
    "        }\n",
    "\n",
    "# Create data collator\n",
    "data_collator = MedicalImagingDataCollator(processor)\n",
    "print(\"‚úÖ Data collator created successfully!\")\n",
    "\n",
    "# Training configuration\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "# Calculate training parameters based on your hardware\n",
    "per_device_batch_size = 1  # Conservative for 12GB VRAM\n",
    "gradient_accumulation_steps = 8  # Simulate larger batch size\n",
    "num_train_epochs = 2  # Start with 2 epochs\n",
    "learning_rate = 2e-5  # Conservative learning rate for medical tasks\n",
    "\n",
    "# Training arguments - FIXED PARAMETER NAMES\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./qwen3vl-bone-fracture\",\n",
    "    num_train_epochs=num_train_epochs,\n",
    "    per_device_train_batch_size=per_device_batch_size,\n",
    "    per_device_eval_batch_size=per_device_batch_size,\n",
    "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
    "    learning_rate=learning_rate,\n",
    "    weight_decay=0.01,\n",
    "    warmup_ratio=0.1,\n",
    "    logging_steps=10,\n",
    "    eval_strategy=\"steps\",  # FIXED: evaluation_strategy -> eval_strategy\n",
    "    eval_steps=50,  # Evaluate every 50 steps\n",
    "    save_strategy=\"steps\",\n",
    "    save_steps=100,  # Save checkpoint every 100 steps\n",
    "    save_total_limit=3,\n",
    "    fp16=True,  # Use mixed precision\n",
    "    bf16=False,\n",
    "    dataloader_num_workers=4,\n",
    "    report_to=\"none\",\n",
    "    logging_first_step=True,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    greater_is_better=False,\n",
    "    optim=\"paged_adamw_8bit\",  # Memory-efficient optimizer\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    seed=42,\n",
    "    remove_unused_columns=False,  # Important for custom collator\n",
    ")\n",
    "\n",
    "print(\"‚úÖ Training configuration created successfully!\")\n",
    "print(f\"Training parameters:\")\n",
    "print(f\"  Batch size per device: {per_device_batch_size}\")\n",
    "print(f\"  Gradient accumulation steps: {gradient_accumulation_steps}\")\n",
    "print(f\"  Effective batch size: {per_device_batch_size * gradient_accumulation_steps}\")\n",
    "print(f\"  Learning rate: {learning_rate}\")\n",
    "print(f\"  Epochs: {num_train_epochs}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b65680",
   "metadata": {},
   "outputs": [],
   "source": [
    "# More robust dataset formatting with explicit feature definitions\n",
    "from datasets import Features, Sequence, Value, Image as ImageFeature\n",
    "\n",
    "def format_for_unsloth_safe(sample):\n",
    "    \"\"\"Safely format samples with error handling for inconsistent data types\"\"\"\n",
    "    try:\n",
    "        # Safely get text input\n",
    "        text_input = str(sample.get(\"text_input\", \"\")).strip()\n",
    "        \n",
    "        # Safely get target output - ensure it's a valid JSON string\n",
    "        target_output = sample.get(\"target_output\", \"\")\n",
    "        if not isinstance(target_output, str):\n",
    "            target_output = json.dumps({})\n",
    "        \n",
    "        # Create messages format\n",
    "        messages = [\n",
    "            {\n",
    "                \"role\": \"user\", \n",
    "                \"content\": [\n",
    "                    {\"type\": \"image\"},\n",
    "                    {\"type\": \"text\", \"text\": text_input}\n",
    "                ]\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": target_output\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        # Safely load image\n",
    "        try:\n",
    "            image_path = sample.get(\"image_path\", \"\")\n",
    "            if not image_path or not os.path.exists(image_path):\n",
    "                # Use a fallback blank image if path is invalid\n",
    "                from PIL import Image\n",
    "                image = Image.new('RGB', (448, 448), color='white')\n",
    "            else:\n",
    "                image = Image.open(image_path).convert(\"RGB\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error loading image {image_path}: {e}\")\n",
    "            from PIL import Image\n",
    "            image = Image.new('RGB', (448, 448), color='white')\n",
    "        \n",
    "        return {\n",
    "            \"messages\": messages,\n",
    "            \"image\": image\n",
    "        }\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error formatting sample: {e}\")\n",
    "        # Return a minimal valid sample as fallback\n",
    "        from PIL import Image\n",
    "        default_image = Image.new('RGB', (448, 448), color='white')\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": [\n",
    "                        {\"type\": \"image\"},\n",
    "                        {\"type\": \"text\", \"text\": \"Analyze this X-ray image for fractures.\"}\n",
    "                    ]\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": \"{}\"\n",
    "                }\n",
    "            ],\n",
    "            \"image\": default_image\n",
    "        }\n",
    "\n",
    "# Define explicit features to avoid Arrow serialization issues\n",
    "features = Features({\n",
    "    \"messages\": Sequence({\n",
    "        \"role\": Value(\"string\"),\n",
    "        \"content\": Value(\"string\"),\n",
    "    }),\n",
    "    \"image\": ImageFeature(),\n",
    "})\n",
    "\n",
    "# Process dataset in smaller batches with progress tracking\n",
    "print(\"\\nüîÑ Reformatting dataset with robust error handling...\")\n",
    "batch_size = 1000\n",
    "total_train = len(dataset[\"train\"])\n",
    "total_eval = len(dataset[\"validation\"])\n",
    "formatted_train_samples = []\n",
    "formatted_eval_samples = []\n",
    "\n",
    "# Process training data in batches\n",
    "for i in range(0, total_train, batch_size):\n",
    "    batch = dataset[\"train\"][i:min(i+batch_size, total_train)]\n",
    "    for j in range(len(batch[\"text_input\"])):\n",
    "        sample = {key: batch[key][j] for key in batch.keys()}\n",
    "        formatted = format_for_unsloth_safe(sample)\n",
    "        formatted_train_samples.append(formatted)\n",
    "    print(f\"  Processed training samples {i}/{total_train}\")\n",
    "\n",
    "# Process validation data in batches\n",
    "for i in range(0, total_eval, batch_size):\n",
    "    batch = dataset[\"validation\"][i:min(i+batch_size, total_eval)]\n",
    "    for j in range(len(batch[\"text_input\"])):\n",
    "        sample = {key: batch[key][j] for key in batch.keys()}\n",
    "        formatted = format_for_unsloth_safe(sample)\n",
    "        formatted_eval_samples.append(formatted)\n",
    "    print(f\"  Processed validation samples {i}/{total_eval}\")\n",
    "\n",
    "# Create datasets with explicit features\n",
    "print(\"\\nüîß Creating dataset objects with explicit features...\")\n",
    "from datasets import Dataset\n",
    "\n",
    "formatted_train = Dataset.from_list(formatted_train_samples, features=features)\n",
    "formatted_eval = Dataset.from_list(formatted_eval_samples, features=features)\n",
    "\n",
    "print(f\"‚úÖ Training samples reformatted: {len(formatted_train)}\")\n",
    "print(f\"‚úÖ Validation samples reformatted: {len(formatted_eval)}\")\n",
    "\n",
    "# Test with a small subset first before full training\n",
    "print(\"\\nüîç Testing data collator with a single sample...\")\n",
    "test_batch = [formatted_train[0]]\n",
    "try:\n",
    "    test_inputs = data_collator(test_batch)\n",
    "    print(\"‚úÖ Data collator test passed!\")\n",
    "    print(f\"  Input IDs shape: {test_inputs['input_ids'].shape}\")\n",
    "    print(f\"  Pixel values shape: {test_inputs['pixel_values'].shape}\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Data collator test failed: {e}\")\n",
    "    # Fallback values if test fails\n",
    "    print(\"‚ö†Ô∏è Using fallback configuration...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efdfc9d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131fc36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aad6808",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6509061e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbabf008",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c64fbd3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "vlm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
